{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4698e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22484cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16701, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/ask_play_station_preprocessed.csv', encoding= 'unicode_escape')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02285ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b645b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc2', ' ', 1)\n",
    "\n",
    "# #data.User = data.User.astype(str)\n",
    "# #data.Company = data.Company.astype(str)\n",
    "# data.Company[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f72a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    User  \\\n",
       " 3556   @AskPlayStation my PS4 keeps putting itself in...   \n",
       " 41     @AskPlayStation I redeemed a VC code for nba 2...   \n",
       " 6992                         @AskPlayStation Nw- 31291-6   \n",
       " 10502  @AskPlayStation Why is it that every time I tr...   \n",
       " 5813   @AskPlayStation when i try to accept the terms...   \n",
       " 11670  @AskPlayStation hi, chat and social thingies d...   \n",
       " 13624  @AskPlayStation the page  it told me to contac...   \n",
       " 9557   @AskPlayStation yo, my ps4 connects to the int...   \n",
       " 10262  @AskPlayStation  My password got randomly chan...   \n",
       " 15941  @AskPlayStation what screw driver size shall i...   \n",
       " \n",
       "                                                  Company  \n",
       " 3556   START @218804 That's not good. Please turn off...  \n",
       " 41     START @117009  No problem. Please follow the s...  \n",
       " 6992   START @376798 Let's check out the next article...  \n",
       " 10502  START @501406 Hi there. Let's look into that. ...  \n",
       " 5813   START @316571 Strange! Are you trying to accep...  \n",
       " 11670  START @598535 That's odd. Please power cycle y...  \n",
       " 13624  START @686898 For further assistance please fo...  \n",
       " 9557   START @344844 Sorry to hear that! Disconnect y...  \n",
       " 10262  START @489219 Happy to help! Please follow us,...  \n",
       " 15941  START @735584 Here to assist! Please check the...  ,\n",
       " 'START @115743 There is no info to share at the moment. Feel free to keep an eye on the PS Blog for news and updates: URL_POSITION END')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in data.index:\n",
    "    data.loc[index,'Company'] = 'START ' + data.loc[index,'Company'] + ' END'\n",
    "data.sample(10), data.Company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ea186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=40)\n",
    "company_ds = tf.data.Dataset.from_tensor_slices(data.Company).batch(128)\n",
    "company_vectorizer.adapt(company_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81455b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=40)\n",
    "user_ds = tf.data.Dataset.from_tensor_slices(data.User).batch(128)\n",
    "user_vectorizer.adapt(user_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e38661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  54,  269,    5, 1235,   57,  133,  304,   35,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = user_vectorizer([\"So, what's the november ps plus free game\"])\n",
    "output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de38834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25,  56,  86,  67,   7, 194, 162,   4, 334, 138, 133,   7, 136,\n",
       "         74, 471,  52,   4, 128, 260,   9, 343,  14, 275,   8,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = company_vectorizer([\"There is no info to share at the moment. Feel free to keep an eye on the PS Blog for news and updates: URL_POSITION\"])\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437a6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company length: 10000\n",
      "User length: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Company length: \" + str(len(company_vectorizer.get_vocabulary())))\n",
    "print(\"User length: \" + str(len(user_vectorizer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd39be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vocabulary = company_vectorizer.get_vocabulary()\n",
    "company_word_index = dict(zip(company_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a87d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocabulary = user_vectorizer.get_vocabulary()\n",
    "user_word_index = dict(zip(user_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9324a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_word_index), len(company_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7091e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 832, 128, 256, 133, 101]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"start\", \"november\", \"ps\", \"plus\", \"free\", \"game\"]\n",
    "[company_word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "206d617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.6B.50d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cab74dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45323 ,  0.059811, -0.10577 , -0.333   ,  0.72359 , -0.08717 ,\n",
       "       -0.61053 , -0.037695, -0.30945 ,  0.21805 , -0.43605 ,  0.47318 ,\n",
       "       -0.76866 , -0.2713  ,  1.1042  ,  0.59141 ,  0.56962 , -0.18678 ,\n",
       "        0.14867 , -0.67292 , -0.34672 ,  0.52284 ,  0.22959 , -0.072014,\n",
       "        0.93967 , -2.3985  , -1.3238  ,  0.28698 ,  0.75509 , -0.76522 ,\n",
       "        3.3425  ,  0.17233 , -0.51803 , -0.8297  , -0.29333 , -0.50076 ,\n",
       "       -0.15228 ,  0.098973,  0.18146 , -0.1742  , -0.40666 ,  0.20348 ,\n",
       "       -0.011788,  0.48252 ,  0.024598,  0.34064 , -0.084724,  0.5324  ,\n",
       "       -0.25103 ,  0.62546 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aa1f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2614 words (7386 misses)\n"
     ]
    }
   ],
   "source": [
    "#Company GloVe embedding\n",
    "\n",
    "company_num_tokens = len(company_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "company_embedding_matrix = np.zeros((company_num_tokens, embedding_dim))\n",
    "for word, i in company_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        company_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a42c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6527 words (3473 misses)\n"
     ]
    }
   ],
   "source": [
    "#User GloVe embedding\n",
    "\n",
    "user_num_tokens = len(user_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "user_embedding_matrix = np.zeros((user_num_tokens, embedding_dim))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83be3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.014226  ,  1.28209996,  0.47413999,  0.029297  ,  0.2624    ,\n",
       "        0.21472   , -0.1075    , -0.38361999,  0.17601   ,  0.13776   ,\n",
       "       -0.38643   , -0.19752   ,  0.42192999, -0.047165  ,  0.56528997,\n",
       "       -0.76681   , -0.077477  ,  0.24017   , -0.24187   , -0.68089002,\n",
       "        0.25938001, -0.40561   ,  0.49706   ,  0.31424001, -1.04999995,\n",
       "       -0.088827  , -0.1934    , -0.24862   ,  0.15663999, -0.04671   ,\n",
       "        3.16820002,  0.76967001,  0.045547  ,  0.95493001,  0.53040999,\n",
       "        0.29933   ,  0.23246001, -0.088557  ,  0.12864999, -0.4375    ,\n",
       "        0.67809999,  0.12878001,  0.48137   , -0.065299  , -0.62515998,\n",
       "        0.040249  ,  0.014061  ,  0.51809001, -0.308     ,  0.62830001])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check user validity\n",
    "user_embedding_matrix[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e61bd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.026071  , -0.14204   ,  0.50678998, -0.38536   , -0.25992   ,\n",
       "        0.061203  , -0.25150001,  0.33658999,  0.10031   ,  0.19701999,\n",
       "       -0.072183  ,  0.13847999, -0.57571   , -0.56156999,  0.63119   ,\n",
       "        1.02530003, -0.51130003, -1.01349998,  0.15967   , -0.39377001,\n",
       "        0.20737   , -0.046717  , -0.38705   , -0.63292998,  0.46724001,\n",
       "       -1.55929995,  0.32508999,  0.46072   ,  0.60162002,  1.28859997,\n",
       "        1.81799996,  0.96003997,  1.30320001, -0.62168998, -0.42491999,\n",
       "       -0.46419999, -1.30859995, -0.88731003,  0.28600001, -0.79233998,\n",
       "       -0.88091999,  0.31139001,  0.28845999,  0.084298  ,  1.25150001,\n",
       "        0.47628   ,  0.39539   ,  1.02499998,  0.28852999, -0.4567    ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check company validity\n",
    "company_embedding_matrix[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4d8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company embedding\n",
    "companny_embedding_layer = Embedding(\n",
    "    company_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(company_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user embedding\n",
    "user_embedding_layer = Embedding(\n",
    "    user_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(user_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "146b9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded_sequences = user_embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(embedding_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedded_sequences)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4504d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedded_sequences = companny_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded_sequences,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(user_num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4b5ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "229bb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     500000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     500000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 50),         20200       ['embedding_1[0][0]']            \n",
      "                                 (None, 50),                                                      \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 50),   20200       ['embedding[0][0]',              \n",
      "                                 (None, 50),                      'lstm[0][1]',                   \n",
      "                                 (None, 50)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 10000)  510000      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,550,400\n",
      "Trainable params: 550,400\n",
      "Non-trainable params: 1,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3de96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = user_vectorizer(np.array([[s] for s in data.User])).numpy()\n",
    "y = company_vectorizer(np.array([[s] for s in data.Company])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6301730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16701, 40), (16701, 40))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eddde91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = []\n",
    "for i in y:\n",
    "    train_y_final_output.append(i[1:])\n",
    "train_y_final_output = pad_sequences(train_y_final_output, 40, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48245ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " array([  1,  25,  56,  86,  67,   7, 194, 162,   4, 334, 138, 133,   7,\n",
       "        136,  74, 471,  52,   4, 128, 260,   9, 343,  14, 275,   8,   3,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]),\n",
       " (16701, 40))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y_final_output[0]), train_y_final_output[0], train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98dd284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = to_categorical(train_y_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e25de013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16701, 40, 10000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9199a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "418/418 [==============================] - 93s 216ms/step - loss: 2.9331 - acc: 0.5898 - val_loss: 2.0079 - val_acc: 0.6380\n",
      "Epoch 2/15\n",
      "418/418 [==============================] - 94s 224ms/step - loss: 1.7556 - acc: 0.6844 - val_loss: 1.5795 - val_acc: 0.7299\n",
      "Epoch 3/15\n",
      "418/418 [==============================] - 96s 230ms/step - loss: 1.4333 - acc: 0.7468 - val_loss: 1.3535 - val_acc: 0.7694\n",
      "Epoch 4/15\n",
      "418/418 [==============================] - 95s 228ms/step - loss: 1.2620 - acc: 0.7738 - val_loss: 1.2441 - val_acc: 0.7875\n",
      "Epoch 5/15\n",
      "418/418 [==============================] - 98s 235ms/step - loss: 1.1614 - acc: 0.7900 - val_loss: 1.1670 - val_acc: 0.8030\n",
      "Epoch 6/15\n",
      "418/418 [==============================] - 98s 235ms/step - loss: 1.0938 - acc: 0.8032 - val_loss: 1.1135 - val_acc: 0.8116\n",
      "Epoch 7/15\n",
      "418/418 [==============================] - 99s 236ms/step - loss: 1.0455 - acc: 0.8127 - val_loss: 1.0870 - val_acc: 0.8175\n",
      "Epoch 8/15\n",
      "418/418 [==============================] - 96s 229ms/step - loss: 1.0054 - acc: 0.8189 - val_loss: 1.0494 - val_acc: 0.8226\n",
      "Epoch 9/15\n",
      "418/418 [==============================] - 93s 222ms/step - loss: 0.9761 - acc: 0.8235 - val_loss: 1.0315 - val_acc: 0.8245\n",
      "Epoch 10/15\n",
      "418/418 [==============================] - 102s 245ms/step - loss: 0.9530 - acc: 0.8270 - val_loss: 1.0261 - val_acc: 0.8279\n",
      "Epoch 11/15\n",
      "418/418 [==============================] - 103s 245ms/step - loss: 0.9355 - acc: 0.8301 - val_loss: 1.0153 - val_acc: 0.8296\n",
      "Epoch 12/15\n",
      "418/418 [==============================] - 100s 239ms/step - loss: 0.9217 - acc: 0.8330 - val_loss: 1.0048 - val_acc: 0.8326\n",
      "Epoch 13/15\n",
      "418/418 [==============================] - 96s 229ms/step - loss: 0.9104 - acc: 0.8358 - val_loss: 0.9984 - val_acc: 0.8349\n",
      "Epoch 14/15\n",
      "418/418 [==============================] - 94s 225ms/step - loss: 0.9008 - acc: 0.8378 - val_loss: 0.9955 - val_acc: 0.8361\n",
      "Epoch 15/15\n",
      "418/418 [==============================] - 96s 231ms/step - loss: 0.8926 - acc: 0.8396 - val_loss: 0.9936 - val_acc: 0.8376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f500bb6d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y], train_y_final_output, epochs = 15, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4abcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= companny_embedding_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5d0ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = user_vectorizer(input_seq)\n",
    "    print(input_seq)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = company_word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = company_vocabulary[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 40):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f226843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2  137    1   95 1409    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 40), dtype=int64)\n",
      " please check your dms for more instructions\n"
     ]
    }
   ],
   "source": [
    "print(decode_sequence([\"@AskPlayStation Thank you💖you are awesome\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf6cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a5cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
