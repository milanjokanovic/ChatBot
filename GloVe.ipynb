{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4698e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22484cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16701, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/ask_play_station_preprocessed.csv', encoding= 'unicode_escape')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02285ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b645b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc2', ' ', 1)\n",
    "\n",
    "# #data.User = data.User.astype(str)\n",
    "# #data.Company = data.Company.astype(str)\n",
    "# data.Company[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f72a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    User  \\\n",
       " 353    @AskPlayStation Is your live chat support down...   \n",
       " 2399   @AskPlayStation I even tried on a different TV...   \n",
       " 3479       @AskPlayStation I can't add a game to my cart   \n",
       " 6155   @AskPlayStation funds were added to my wallet ...   \n",
       " 3314   @AskPlayStation Wireless connection. Error cod...   \n",
       " 9202   @AskPlayStation Everytime I try to log into my...   \n",
       " 3688   @AskPlayStation what is going on with my servi...   \n",
       " 15194  @AskPlayStation @117014 We just buy a PS4, and...   \n",
       " 5644   @AskPlayStation changed my password I have 2 s...   \n",
       " 13834  @AskPlayStation my ps4 won't let me play any o...   \n",
       " \n",
       "                                                  Company  \n",
       " 353    START @135663 Odd! We have sent you a Direct M...  \n",
       " 2399   START @197938 That's not good. Please check yo...  \n",
       " 3479   START @217676 That's odd. Try making the purch...  \n",
       " 6155   START @346390 Happy to help! For refund info, ...  \n",
       " 3314   START @212973 No worries! We appreciate the up...  \n",
       " 9202   START @450221 Hey James. Please check your DM'...  \n",
       " 3688   START @226685 Sorry for the delay Jeremy. Can ...  \n",
       " 15194  START @758381 That's odd. Try making the purch...  \n",
       " 5644   START @327467 Understood! We have sent you a D...  \n",
       " 13834  START @708975 Hi Laura, Glad to assist! Make s...  ,\n",
       " 'START @115743 There is no info to share at the moment. Feel free to keep an eye on the PS Blog for news and updates: URL_POSITION END')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in data.index:\n",
    "    data.loc[index,'Company'] = 'START ' + data.loc[index,'Company'] + ' END'\n",
    "data.sample(10), data.Company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ea186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=40)\n",
    "company_ds = tf.data.Dataset.from_tensor_slices(data.Company).batch(128)\n",
    "company_vectorizer.adapt(company_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81455b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=40)\n",
    "user_ds = tf.data.Dataset.from_tensor_slices(data.User).batch(128)\n",
    "user_vectorizer.adapt(user_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  54,  269,    5, 1235,   57,  133,  304,   35,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = user_vectorizer([\"So, what's the november ps plus free game\"])\n",
    "output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de38834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25,  56,  86,  67,   7, 194, 162,   4, 334, 138, 133,   7, 136,\n",
       "         74, 471,  52,   4, 128, 260,   9, 343,  14, 275,   8,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = company_vectorizer([\"There is no info to share at the moment. Feel free to keep an eye on the PS Blog for news and updates: URL_POSITION\"])\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437a6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company length: 7000\n",
      "User length: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Company length: \" + str(len(company_vectorizer.get_vocabulary())))\n",
    "print(\"User length: \" + str(len(user_vectorizer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd39be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vocabulary = company_vectorizer.get_vocabulary()\n",
    "company_word_index = dict(zip(company_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a87d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocabulary = user_vectorizer.get_vocabulary()\n",
    "user_word_index = dict(zip(user_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9324a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 7000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_word_index), len(company_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7091e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 832, 128, 256, 133, 101]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"start\", \"november\", \"ps\", \"plus\", \"free\", \"game\"]\n",
    "[company_word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "206d617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('C:/Users/Aleksandar/Desktop/glove/glove.6B.50d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cab74dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45323 ,  0.059811, -0.10577 , -0.333   ,  0.72359 , -0.08717 ,\n",
       "       -0.61053 , -0.037695, -0.30945 ,  0.21805 , -0.43605 ,  0.47318 ,\n",
       "       -0.76866 , -0.2713  ,  1.1042  ,  0.59141 ,  0.56962 , -0.18678 ,\n",
       "        0.14867 , -0.67292 , -0.34672 ,  0.52284 ,  0.22959 , -0.072014,\n",
       "        0.93967 , -2.3985  , -1.3238  ,  0.28698 ,  0.75509 , -0.76522 ,\n",
       "        3.3425  ,  0.17233 , -0.51803 , -0.8297  , -0.29333 , -0.50076 ,\n",
       "       -0.15228 ,  0.098973,  0.18146 , -0.1742  , -0.40666 ,  0.20348 ,\n",
       "       -0.011788,  0.48252 ,  0.024598,  0.34064 , -0.084724,  0.5324  ,\n",
       "       -0.25103 ,  0.62546 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa1f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2607 words (4393 misses)\n"
     ]
    }
   ],
   "source": [
    "#Company GloVe embedding\n",
    "\n",
    "company_num_tokens = len(company_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "company_embedding_matrix = np.zeros((company_num_tokens, embedding_dim))\n",
    "for word, i in company_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        company_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a42c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5073 words (1927 misses)\n"
     ]
    }
   ],
   "source": [
    "#User GloVe embedding\n",
    "\n",
    "user_num_tokens = len(user_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "user_embedding_matrix = np.zeros((user_num_tokens, embedding_dim))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83be3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.014226  ,  1.28209996,  0.47413999,  0.029297  ,  0.2624    ,\n",
       "        0.21472   , -0.1075    , -0.38361999,  0.17601   ,  0.13776   ,\n",
       "       -0.38643   , -0.19752   ,  0.42192999, -0.047165  ,  0.56528997,\n",
       "       -0.76681   , -0.077477  ,  0.24017   , -0.24187   , -0.68089002,\n",
       "        0.25938001, -0.40561   ,  0.49706   ,  0.31424001, -1.04999995,\n",
       "       -0.088827  , -0.1934    , -0.24862   ,  0.15663999, -0.04671   ,\n",
       "        3.16820002,  0.76967001,  0.045547  ,  0.95493001,  0.53040999,\n",
       "        0.29933   ,  0.23246001, -0.088557  ,  0.12864999, -0.4375    ,\n",
       "        0.67809999,  0.12878001,  0.48137   , -0.065299  , -0.62515998,\n",
       "        0.040249  ,  0.014061  ,  0.51809001, -0.308     ,  0.62830001])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check user validity\n",
    "user_embedding_matrix[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e61bd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.026071  , -0.14204   ,  0.50678998, -0.38536   , -0.25992   ,\n",
       "        0.061203  , -0.25150001,  0.33658999,  0.10031   ,  0.19701999,\n",
       "       -0.072183  ,  0.13847999, -0.57571   , -0.56156999,  0.63119   ,\n",
       "        1.02530003, -0.51130003, -1.01349998,  0.15967   , -0.39377001,\n",
       "        0.20737   , -0.046717  , -0.38705   , -0.63292998,  0.46724001,\n",
       "       -1.55929995,  0.32508999,  0.46072   ,  0.60162002,  1.28859997,\n",
       "        1.81799996,  0.96003997,  1.30320001, -0.62168998, -0.42491999,\n",
       "       -0.46419999, -1.30859995, -0.88731003,  0.28600001, -0.79233998,\n",
       "       -0.88091999,  0.31139001,  0.28845999,  0.084298  ,  1.25150001,\n",
       "        0.47628   ,  0.39539   ,  1.02499998,  0.28852999, -0.4567    ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check company validity\n",
    "company_embedding_matrix[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4d8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company embedding\n",
    "companny_embedding_layer = Embedding(\n",
    "    company_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(company_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user embedding\n",
    "user_embedding_layer = Embedding(\n",
    "    user_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(user_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "146b9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded_sequences = user_embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(embedding_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedded_sequences)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4504d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedded_sequences = companny_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded_sequences,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(user_num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b5ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "229bb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     350000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     350000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 50),         20200       ['embedding_1[0][0]']            \n",
      "                                 (None, 50),                                                      \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 50),   20200       ['embedding[0][0]',              \n",
      "                                 (None, 50),                      'lstm[0][1]',                   \n",
      "                                 (None, 50)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 7000)   357000      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,097,400\n",
      "Trainable params: 397,400\n",
      "Non-trainable params: 700,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3de96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = user_vectorizer(np.array([[s] for s in data.User])).numpy()\n",
    "y = company_vectorizer(np.array([[s] for s in data.Company])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6301730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16701, 40), (16701, 40))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eddde91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = []\n",
    "for i in y:\n",
    "    train_y_final_output.append(i[1:])\n",
    "train_y_final_output = pad_sequences(train_y_final_output, 40, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48245ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " array([  1,  25,  56,  86,  67,   7, 194, 162,   4, 334, 138, 133,   7,\n",
       "        136,  74, 471,  52,   4, 128, 260,   9, 343,  14, 275,   8,   3,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]),\n",
       " (16701, 40))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y_final_output[0]), train_y_final_output[0], train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98dd284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = to_categorical(train_y_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e25de013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16701, 40, 7000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9199a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "418/418 [==============================] - 182s 413ms/step - loss: 2.8711 - acc: 0.5876 - val_loss: 1.9248 - val_acc: 0.6496\n",
      "Epoch 2/3\n",
      "418/418 [==============================] - 93s 219ms/step - loss: 1.7197 - acc: 0.6870 - val_loss: 1.5015 - val_acc: 0.7363\n",
      "Epoch 3/3\n",
      "418/418 [==============================] - 113s 272ms/step - loss: 1.4133 - acc: 0.7455 - val_loss: 1.2890 - val_acc: 0.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b7680c730>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y], train_y_final_output, epochs = 3, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4abcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= companny_embedding_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5d0ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = user_vectorizer(input_seq)\n",
    "    print(input_seq)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = company_word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = company_vocabulary[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 40):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f226843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2  26 338   6  16 299   5  65 137  21  54 386   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]], shape=(1, 40), dtype=int64)\n",
      " [UNK] please check your dms for further instructions\n"
     ]
    }
   ],
   "source": [
    "print(decode_sequence([\"@AskPlayStation That seems to have fixed the problem. Thank you so much.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf6cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a5cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
