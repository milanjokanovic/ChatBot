{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d406b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, GRU, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a679d2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18668, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/mixed_data_preprocessed_fixed.csv', encoding= 'unicode_escape')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8555538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c77e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc3', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc3', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace(' â\\x89\\xa0 ', ' ', 1)\n",
    "        row[1] = str(row[1]).replace(' â\\x89\\xa0 ', ' ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae928e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                 Company  \\\n",
       " 4538   START @143223 Jim, did you make a delayed bagg...   \n",
       " 15068  START @120961 Can you try uninstalling the App...   \n",
       " 13074  START @152376 Hello, would you be able to dire...   \n",
       " 6315   START @138976 We're unable to give you a ticke...   \n",
       " 14127  START @118023 That's not what we like to hear,...   \n",
       " 7324   START @148368 We have deals on deal on deals, ...   \n",
       " 15017  START @155030 Sorry for any frustration with y...   \n",
       " 7736   START @125954 Hey, there. I would be happy to ...   \n",
       " 4547   START @143226 You are very welcome. Enjoy your...   \n",
       " 3177   START @127424 Hey! Can you DM us your account'...   \n",
       " \n",
       "                                                     User  \n",
       " 4538   @Delta that being said, the young lady that to...  \n",
       " 15068  @sainsburys pls help. App isn't working. Check...  \n",
       " 13074  @XboxSupport I'm an Xbox gold member but I can...  \n",
       " 6315   @Delta @AmericanAir ok whoever gives me a plan...  \n",
       " 14127  @hulu_support Cannot access HULU for the past ...  \n",
       " 7324   @sprintcare Not what I asked. It was upgrading...  \n",
       " 15017      Got it all set up. NO @hulu_support APP. ð¤¬  \n",
       " 7736   Oh look.  Another @115900 outage in my area.  ...  \n",
       " 4547                    @Delta Ok, thanks for confirming  \n",
       " 3177   uhh so somehow i canât even log into my spot...  ,\n",
       " \"START @115820 I'm sorry we've let you down! Without providing any personal information, will you describe the issue? We'd love to help. ^TN END\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in data.index:\n",
    "    data.loc[index,'Company'] = 'START ' + data.loc[index,'Company'] + ' END'\n",
    "data.sample(10), data.Company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3aefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b75dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=40)\n",
    "company_ds = tf.data.Dataset.from_tensor_slices(train_data.Company).batch(128)\n",
    "company_vectorizer.adapt(company_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad6969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=40)\n",
    "user_ds = tf.data.Dataset.from_tensor_slices(train_data.User).batch(128)\n",
    "user_vectorizer.adapt(user_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13ad215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company length: 7000\n",
      "User length: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Company length: \" + str(len(company_vectorizer.get_vocabulary())))\n",
    "print(\"User length: \" + str(len(user_vectorizer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vocabulary = company_vectorizer.get_vocabulary()\n",
    "company_word_index = dict(zip(company_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be587254",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocabulary = user_vectorizer.get_vocabulary()\n",
    "user_word_index = dict(zip(user_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba8944d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 7000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_word_index), len(company_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0627807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.6B.50d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c017d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4465 words (2535 misses)\n"
     ]
    }
   ],
   "source": [
    "#Company GloVe embedding\n",
    "\n",
    "company_num_tokens = len(company_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "company_embedding_matrix = np.zeros((company_num_tokens, embedding_dim))\n",
    "for word, i in company_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        company_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b48cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6167 words (833 misses)\n"
     ]
    }
   ],
   "source": [
    "#User GloVe embedding\n",
    "\n",
    "user_num_tokens = len(user_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "user_embedding_matrix = np.zeros((user_num_tokens, embedding_dim))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6ae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company embedding\n",
    "companny_embedding_layer = Embedding(\n",
    "    company_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(company_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4266f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user embedding\n",
    "user_embedding_layer = Embedding(\n",
    "    user_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(user_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e22a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded_sequences = user_embedding_layer(encoder_inputs)\n",
    "encoder_gru = GRU(embedding_dim, return_state=True)\n",
    "encoder_outputs, encoder_states = encoder_gru(encoder_embedded_sequences)\n",
    "#encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14be0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedded_sequences = companny_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(decoder_embedded_sequences,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(user_num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb7f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     350000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     350000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru (GRU)                      [(None, 50),         15300       ['embedding_1[0][0]']            \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, None, 50),   15300       ['embedding[0][0]',              \n",
      "                                 (None, 50)]                      'gru[0][1]']                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 7000)   357000      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,087,600\n",
      "Trainable params: 387,600\n",
      "Non-trainable params: 700,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d55fb49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13067, 40), (13067, 40))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = user_vectorizer(np.array([[s] for s in train_data.User])).numpy()\n",
    "y = company_vectorizer(np.array([[s] for s in train_data.Company])).numpy()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85f7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = []\n",
    "for i in y:\n",
    "    train_y_final_output.append(i[1:])\n",
    "train_y_final_output = pad_sequences(train_y_final_output, 40, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4429a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13067, 40, 7000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_final_output = to_categorical(train_y_final_output)\n",
    "train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15685d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "327/327 [==============================] - 55s 156ms/step - loss: 3.9580 - acc: 0.4807 - val_loss: 3.0724 - val_acc: 0.5159\n",
      "Epoch 2/20\n",
      "327/327 [==============================] - 46s 142ms/step - loss: 2.9311 - acc: 0.5386 - val_loss: 2.8256 - val_acc: 0.5540\n",
      "Epoch 3/20\n",
      "327/327 [==============================] - 58s 179ms/step - loss: 2.7369 - acc: 0.5638 - val_loss: 2.6874 - val_acc: 0.5712\n",
      "Epoch 4/20\n",
      "327/327 [==============================] - 50s 154ms/step - loss: 2.6210 - acc: 0.5782 - val_loss: 2.5961 - val_acc: 0.5834\n",
      "Epoch 5/20\n",
      "327/327 [==============================] - 48s 147ms/step - loss: 2.5406 - acc: 0.5884 - val_loss: 2.5325 - val_acc: 0.5910\n",
      "Epoch 6/20\n",
      "327/327 [==============================] - 49s 150ms/step - loss: 2.4803 - acc: 0.5947 - val_loss: 2.4855 - val_acc: 0.5966\n",
      "Epoch 7/20\n",
      "327/327 [==============================] - 50s 154ms/step - loss: 2.4336 - acc: 0.5995 - val_loss: 2.4508 - val_acc: 0.6002\n",
      "Epoch 8/20\n",
      "327/327 [==============================] - 49s 151ms/step - loss: 2.3960 - acc: 0.6039 - val_loss: 2.4191 - val_acc: 0.6046\n",
      "Epoch 9/20\n",
      "327/327 [==============================] - 49s 151ms/step - loss: 2.3654 - acc: 0.6074 - val_loss: 2.3961 - val_acc: 0.6071\n",
      "Epoch 10/20\n",
      "327/327 [==============================] - 50s 152ms/step - loss: 2.3392 - acc: 0.6104 - val_loss: 2.3776 - val_acc: 0.6090\n",
      "Epoch 11/20\n",
      "327/327 [==============================] - 45s 137ms/step - loss: 2.3166 - acc: 0.6124 - val_loss: 2.3604 - val_acc: 0.6104\n",
      "Epoch 12/20\n",
      "327/327 [==============================] - 50s 154ms/step - loss: 2.2971 - acc: 0.6146 - val_loss: 2.3463 - val_acc: 0.6117\n",
      "Epoch 13/20\n",
      "327/327 [==============================] - 49s 151ms/step - loss: 2.2798 - acc: 0.6164 - val_loss: 2.3347 - val_acc: 0.6143\n",
      "Epoch 14/20\n",
      "327/327 [==============================] - 48s 146ms/step - loss: 2.2643 - acc: 0.6184 - val_loss: 2.3217 - val_acc: 0.6155\n",
      "Epoch 15/20\n",
      "327/327 [==============================] - 48s 147ms/step - loss: 2.2503 - acc: 0.6202 - val_loss: 2.3109 - val_acc: 0.6167\n",
      "Epoch 16/20\n",
      "327/327 [==============================] - 48s 147ms/step - loss: 2.2375 - acc: 0.6214 - val_loss: 2.3008 - val_acc: 0.6186\n",
      "Epoch 17/20\n",
      "327/327 [==============================] - 48s 148ms/step - loss: 2.2256 - acc: 0.6229 - val_loss: 2.2932 - val_acc: 0.6191\n",
      "Epoch 18/20\n",
      "327/327 [==============================] - 53s 164ms/step - loss: 2.2146 - acc: 0.6240 - val_loss: 2.2851 - val_acc: 0.6204\n",
      "Epoch 19/20\n",
      "327/327 [==============================] - 50s 154ms/step - loss: 2.2043 - acc: 0.6254 - val_loss: 2.2778 - val_acc: 0.6211\n",
      "Epoch 20/20\n",
      "327/327 [==============================] - 44s 135ms/step - loss: 2.1948 - acc: 0.6266 - val_loss: 2.2718 - val_acc: 0.6223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211eb5fc310>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y], train_y_final_output, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e0bfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights/mixed_gru_glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e380c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "#decoder_state_input_h = Input(shape=(50,))\n",
    "#decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = Input(shape=(50,))#[decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= companny_embedding_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, decoder_states2 = decoder_gru(dec_emb2, initial_state=decoder_states_inputs)\n",
    "#decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_states_inputs],\n",
    "    [decoder_outputs2] + [decoder_states2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18eb13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = user_vectorizer(input_seq)\n",
    "    #print(input_seq)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = company_word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    repeat = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, states_value = decoder_model.predict([target_seq] + [states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = company_vocabulary[sampled_token_index]\n",
    "        prev = decoded_sentence\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (prev.rstrip() == decoded_sentence.rstrip()):\n",
    "            repeat = repeat + 1\n",
    "        else:\n",
    "            repeat = 0\n",
    "        \n",
    "        if (sampled_char == 'end' or\n",
    "           len(decoded_sentence) > 40):\n",
    "            stop_condition = True\n",
    "        if repeat > 5:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        #states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b2f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there sorry to hear this was the\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi [UNK] sorry to hear this is the\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we are working to help with your internet\n",
      " [UNK] we are working to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we are working to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we are happy to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi [UNK] sorry to hear this is not\n",
      " [UNK] hi [UNK] sorry to hear this is the\n",
      " [UNK] hi there sorry to hear this is the\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there sorry to hear this is the\n",
      " [UNK] we are able to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are working to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are working to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi [UNK] sorry to hear this is the\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we are happy to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] hi there sorry to hear this was the\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to help with your internet\n",
      " [UNK] we can help with this issue please\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] i apologize for the inconvenience please\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] we are able to the best of the service\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to the best of service\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we are able to help with your internet\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we are able to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we can help with your internet issues\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] hey there can you dm us your accounts\n",
      " [UNK] hi there can you dm us your accounts\n",
      " [UNK] we want to help with your internet\n",
      " [UNK] we want to help with your internet\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data[:200].iterrows():\n",
    "    print(decode_sequence([row['User']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7cad63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
