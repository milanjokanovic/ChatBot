{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d406b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, GRU, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a679d2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18668, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/mixed_data_preprocessed_fixed.csv', encoding= 'unicode_escape')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8555538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c77e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc2', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\0xc3', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\0xc3', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Â\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Â\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace(' â\\x89\\xa0 ', ' ', 1)\n",
    "        row[1] = str(row[1]).replace(' â\\x89\\xa0 ', ' ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae928e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                 Company  \\\n",
       " 17142  START @124392 Hey Alex. You can deactivate you...   \n",
       " 387    START @119452 Apologies for the trouble with t...   \n",
       " 1343   START @123054 Battery life is surely important...   \n",
       " 6263   START @138455 It was our pleasure and first ha...   \n",
       " 15556  START @131755 Hi there, what's your local stor...   \n",
       " 16720  START @142393 Hi Conal. How can I help? Rachel...   \n",
       " 10196  START @150456 Our pleasure! Hope to see you bo...   \n",
       " 2286   START @123202 Hi there! Please reach out to UR...   \n",
       " 14869  START @144533 Hmm... That is a bit odd! If: UR...   \n",
       " 11323  START @118444 I will be glad to take a look at...   \n",
       " \n",
       "                                                     User  \n",
       " 17142  @AskPlayStation I got my ps4 stolen so I bough...  \n",
       " 387    @AmazonHelp How many times will the pick-up be...  \n",
       " 1343   Hey @115858 can you stop focusing on adding mo...  \n",
       " 6263   Thanks @americanair for a nice trip to BHM and...  \n",
       " 15556  Does anyone know where I can get one of these ...  \n",
       " 16720  @GWRHelp @GWRHelp any explanation greatly appr...  \n",
       " 10196  Okay just picked my son up from the airport. T...  \n",
       " 2286   @Uber_Support I have just had the worst experi...  \n",
       " 14869  @hulu_support I was using hulu just yesterday,...  \n",
       " 11323  @Ask_Spectrum No, can you just turn the intern...  ,\n",
       " \"START @115820 I'm sorry we've let you down! Without providing any personal information, will you describe the issue? We'd love to help. ^TN END\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in data.index:\n",
    "    data.loc[index,'Company'] = 'START ' + data.loc[index,'Company'] + ' END'\n",
    "data.sample(10), data.Company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3aefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b75dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=20)\n",
    "company_ds = tf.data.Dataset.from_tensor_slices(train_data.Company).batch(128)\n",
    "company_vectorizer.adapt(company_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad6969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=20)\n",
    "user_ds = tf.data.Dataset.from_tensor_slices(train_data.User).batch(128)\n",
    "user_vectorizer.adapt(user_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13ad215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company length: 7000\n",
      "User length: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Company length: \" + str(len(company_vectorizer.get_vocabulary())))\n",
    "print(\"User length: \" + str(len(user_vectorizer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vocabulary = company_vectorizer.get_vocabulary()\n",
    "company_word_index = dict(zip(company_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be587254",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocabulary = user_vectorizer.get_vocabulary()\n",
    "user_word_index = dict(zip(user_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba8944d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 7000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_word_index), len(company_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0627807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.6B.50d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c017d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4465 words (2535 misses)\n"
     ]
    }
   ],
   "source": [
    "#Company GloVe embedding\n",
    "\n",
    "company_num_tokens = len(company_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "company_embedding_matrix = np.zeros((company_num_tokens, embedding_dim))\n",
    "for word, i in company_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        company_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b48cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6167 words (833 misses)\n"
     ]
    }
   ],
   "source": [
    "#User GloVe embedding\n",
    "\n",
    "user_num_tokens = len(user_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "user_embedding_matrix = np.zeros((user_num_tokens, embedding_dim))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6ae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company embedding\n",
    "companny_embedding_layer = Embedding(\n",
    "    company_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(company_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4266f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user embedding\n",
    "user_embedding_layer = Embedding(\n",
    "    user_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(user_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e22a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded_sequences = user_embedding_layer(encoder_inputs)\n",
    "encoder_gru = GRU(embedding_dim, return_state=True)\n",
    "encoder_outputs, encoder_states = encoder_gru(encoder_embedded_sequences)\n",
    "#encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14be0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedded_sequences = companny_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(decoder_embedded_sequences,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(user_num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb7f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     350000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     350000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru (GRU)                      [(None, 50),         15300       ['embedding_1[0][0]']            \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, None, 50),   15300       ['embedding[0][0]',              \n",
      "                                 (None, 50)]                      'gru[0][1]']                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 7000)   357000      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,087,600\n",
      "Trainable params: 387,600\n",
      "Non-trainable params: 700,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d55fb49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13067, 20), (13067, 20))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = user_vectorizer(np.array([[s] for s in train_data.User])).numpy()\n",
    "y = company_vectorizer(np.array([[s] for s in train_data.Company])).numpy()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85f7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = []\n",
    "for i in y:\n",
    "    train_y_final_output.append(i[1:])\n",
    "train_y_final_output = pad_sequences(train_y_final_output, 20, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4429a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13067, 20, 7000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_final_output = to_categorical(train_y_final_output)\n",
    "train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15685d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "327/327 [==============================] - 24s 61ms/step - loss: 5.6206 - acc: 0.1511 - val_loss: 5.0125 - val_acc: 0.2114\n",
      "Epoch 2/20\n",
      "327/327 [==============================] - 19s 57ms/step - loss: 4.8134 - acc: 0.2360 - val_loss: 4.6501 - val_acc: 0.2606\n",
      "Epoch 3/20\n",
      "327/327 [==============================] - 18s 57ms/step - loss: 4.5208 - acc: 0.2767 - val_loss: 4.4358 - val_acc: 0.2901\n",
      "Epoch 4/20\n",
      "327/327 [==============================] - 18s 56ms/step - loss: 4.3357 - acc: 0.2996 - val_loss: 4.2928 - val_acc: 0.3110\n",
      "Epoch 5/20\n",
      "327/327 [==============================] - 18s 56ms/step - loss: 4.2044 - acc: 0.3156 - val_loss: 4.1876 - val_acc: 0.3240\n",
      "Epoch 6/20\n",
      "327/327 [==============================] - 18s 57ms/step - loss: 4.1065 - acc: 0.3283 - val_loss: 4.1060 - val_acc: 0.3343\n",
      "Epoch 7/20\n",
      "327/327 [==============================] - 19s 57ms/step - loss: 4.0278 - acc: 0.3375 - val_loss: 4.0491 - val_acc: 0.3405\n",
      "Epoch 8/20\n",
      "327/327 [==============================] - 19s 58ms/step - loss: 3.9642 - acc: 0.3442 - val_loss: 3.9961 - val_acc: 0.3469\n",
      "Epoch 9/20\n",
      "327/327 [==============================] - 19s 58ms/step - loss: 3.9131 - acc: 0.3502 - val_loss: 3.9583 - val_acc: 0.3509\n",
      "Epoch 10/20\n",
      "327/327 [==============================] - 19s 59ms/step - loss: 3.8703 - acc: 0.3552 - val_loss: 3.9223 - val_acc: 0.3547\n",
      "Epoch 11/20\n",
      "327/327 [==============================] - 19s 59ms/step - loss: 3.8331 - acc: 0.3591 - val_loss: 3.9007 - val_acc: 0.3570\n",
      "Epoch 12/20\n",
      "327/327 [==============================] - 19s 59ms/step - loss: 3.8007 - acc: 0.3632 - val_loss: 3.8718 - val_acc: 0.3618\n",
      "Epoch 13/20\n",
      "327/327 [==============================] - 20s 60ms/step - loss: 3.7726 - acc: 0.3667 - val_loss: 3.8888 - val_acc: 0.3559\n",
      "Epoch 14/20\n",
      "327/327 [==============================] - 19s 60ms/step - loss: 3.7463 - acc: 0.3698 - val_loss: 3.8340 - val_acc: 0.3660\n",
      "Epoch 15/20\n",
      "327/327 [==============================] - 19s 60ms/step - loss: 3.7250 - acc: 0.3729 - val_loss: 3.8266 - val_acc: 0.3687\n",
      "Epoch 16/20\n",
      "327/327 [==============================] - 20s 60ms/step - loss: 3.7038 - acc: 0.3754 - val_loss: 3.8005 - val_acc: 0.3723\n",
      "Epoch 17/20\n",
      "327/327 [==============================] - 20s 60ms/step - loss: 3.6854 - acc: 0.3775 - val_loss: 3.7902 - val_acc: 0.3749\n",
      "Epoch 18/20\n",
      "327/327 [==============================] - 20s 60ms/step - loss: 3.6690 - acc: 0.3795 - val_loss: 3.7742 - val_acc: 0.3768\n",
      "Epoch 19/20\n",
      "327/327 [==============================] - 23s 71ms/step - loss: 3.6530 - acc: 0.3820 - val_loss: 3.7715 - val_acc: 0.3748\n",
      "Epoch 20/20\n",
      "327/327 [==============================] - 20s 62ms/step - loss: 3.6385 - acc: 0.3834 - val_loss: 3.7549 - val_acc: 0.3789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15782922580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y], train_y_final_output, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0bfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights/mixed_gru_glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e380c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "#decoder_state_input_h = Input(shape=(50,))\n",
    "#decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = Input(shape=(50,))#[decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= companny_embedding_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, decoder_states2 = decoder_gru(dec_emb2, initial_state=decoder_states_inputs)\n",
    "#decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_states_inputs],\n",
    "    [decoder_outputs2] + [decoder_states2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18eb13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = user_vectorizer(input_seq)\n",
    "    #print(input_seq)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = company_word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    repeat = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, states_value = decoder_model.predict([target_seq] + [states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = company_vocabulary[sampled_token_index]\n",
    "        prev = decoded_sentence\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (prev.rstrip() == decoded_sentence.rstrip()):\n",
    "            repeat = repeat + 1\n",
    "        else:\n",
    "            repeat = 0\n",
    "        \n",
    "        if (sampled_char == 'end' or\n",
    "           len(decoded_sentence) > 20):\n",
    "            stop_condition = True\n",
    "        if repeat > 2:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        #states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b2f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there we can\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] we want to help\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we want to help\n",
      " [UNK] hi there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we want to help\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] we want to help\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there please\n",
      " [UNK] we can help with\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] we want to help\n",
      " [UNK] we want to help\n",
      " [UNK] hi there we can\n",
      " [UNK] we want to help\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we want to help\n",
      " [UNK] hey there can you\n",
      " [UNK] we want to help\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] we have a great\n",
      " [UNK] hi there can you\n",
      " [UNK] hi [UNK] im sorry\n",
      " [UNK] hi there please\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there we can\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we want to help\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] we want to help\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] we appreciate your\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we want to help\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we want to help\n",
      " [UNK] we want to help\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data[:200].iterrows():\n",
    "    print(decode_sequence([row['User']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65bd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
