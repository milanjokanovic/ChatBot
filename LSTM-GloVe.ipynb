{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4698e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22484cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18668, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv('dataset/ask_play_station_preprocessed.csv', encoding= 'unicode_escape')\n",
    "data = pd.read_csv('dataset/mixed_data_preprocessed_fixed.csv', encoding= 'unicode_escape')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02285ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b645b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        row[0] = str(row[0]).replace('Ã‚\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Ã‚\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Ã‚\\0xc2', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Ã‚\\0xc2', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Ã‚\\0xc3', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Ã‚\\0xc3', ' ', 1)\n",
    "        row[0] = str(row[0]).replace('Ã‚\\xa0', ' ', 1)\n",
    "        row[1] = str(row[1]).replace('Ã‚\\xa0', ' ', 1)\n",
    "        row[0] = str(row[0]).replace(' Ã¢\\x89\\xa0 ', ' ', 1)\n",
    "        row[1] = str(row[1]).replace(' Ã¢\\x89\\xa0 ', ' ', 1)\n",
    "\n",
    "# #data.User = data.User.astype(str)\n",
    "# #data.Company = data.Company.astype(str)\n",
    "#data.Company[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f72a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                 Company  \\\n",
       " 18444        START @118570 Thanks for the kudos! -AC END   \n",
       " 9586   START @124040 Thanks, Katy! We will share you ...   \n",
       " 4411   START @138981 I encourage you to contact them ...   \n",
       " 9831   START @130455 We're looking forward to having ...   \n",
       " 9111   START @133466 Hi Chevy. We hope you have a gre...   \n",
       " 391    START @119532 Thanks for keeping us posted. I'...   \n",
       " 2663   START @130279 Riders are selected at random fo...   \n",
       " 15698  START @133736 Hi Neil, could you confirm which...   \n",
       " 3366   START @131002 Hmm. Can you tell us when this s...   \n",
       " 15316  START @124384 You're very welcome! Please be a...   \n",
       " \n",
       "                                                     User  \n",
       " 18444  @3226 @ChipotleTweets Becky was a first rate h...  \n",
       " 9586   @SouthwestAir Of course! It was Flight 278 CLE...  \n",
       " 4411   @Delta I did. They wonÃ¢Â€Â™t help. I did not see...  \n",
       " 9831   Always awesome flying home to Denver over the ...  \n",
       " 9111   About to fly to London with @British_Airways !...  \n",
       " 391    Thanks @AmazonHelp for solving my issue . But ...  \n",
       " 2663   @Uber_Support hi, what are the qualifications ...  \n",
       " 15698  @sainsburys You need to get your act together ...  \n",
       " 3366                       @SpotifyCares Saved playlists  \n",
       " 15316  @sainsburys Thanks - it happened already befor...  ,\n",
       " \"START @115820 I'm sorry we've let you down! Without providing any personal information, will you describe the issue? We'd love to help. ^TN END\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in data.index:\n",
    "    data.loc[index,'Company'] = 'START ' + data.loc[index,'Company'] + ' END'\n",
    "data.sample(10), data.Company[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0bc5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ea186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=20)\n",
    "company_ds = tf.data.Dataset.from_tensor_slices(train_data.Company).batch(128)\n",
    "company_vectorizer.adapt(company_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81455b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectorizer = TextVectorization(max_tokens=7000, output_sequence_length=20)\n",
    "user_ds = tf.data.Dataset.from_tensor_slices(train_data.User).batch(128)\n",
    "user_vectorizer.adapt(user_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e38661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  32,  292,    2, 1056,  625,  485,  206,  208,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = user_vectorizer([\"So, what's the november ps plus free game\"])\n",
    "output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de38834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  36,   26,   92,   98,    5,  145,   44,    6,  295,  174,  194,\n",
       "           5,  184,   80,  591,   19,    6, 1024, 2452,    8]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = company_vectorizer([\"There is no info to share at the moment. Feel free to keep an eye on the PS Blog for news and updates: URL_POSITION\"])\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437a6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company length: 7000\n",
      "User length: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Company length: \" + str(len(company_vectorizer.get_vocabulary())))\n",
    "print(\"User length: \" + str(len(user_vectorizer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd39be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vocabulary = company_vectorizer.get_vocabulary()\n",
    "company_word_index = dict(zip(company_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a87d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocabulary = user_vectorizer.get_vocabulary()\n",
    "user_word_index = dict(zip(user_vocabulary, range(len(company_vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9324a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 7000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_word_index), len(company_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7091e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1027, 1024, 1119, 194, 411]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"start\", \"november\", \"ps\", \"plus\", \"free\", \"game\"]\n",
    "[company_word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206d617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.6B.50d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cab74dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45323 ,  0.059811, -0.10577 , -0.333   ,  0.72359 , -0.08717 ,\n",
       "       -0.61053 , -0.037695, -0.30945 ,  0.21805 , -0.43605 ,  0.47318 ,\n",
       "       -0.76866 , -0.2713  ,  1.1042  ,  0.59141 ,  0.56962 , -0.18678 ,\n",
       "        0.14867 , -0.67292 , -0.34672 ,  0.52284 ,  0.22959 , -0.072014,\n",
       "        0.93967 , -2.3985  , -1.3238  ,  0.28698 ,  0.75509 , -0.76522 ,\n",
       "        3.3425  ,  0.17233 , -0.51803 , -0.8297  , -0.29333 , -0.50076 ,\n",
       "       -0.15228 ,  0.098973,  0.18146 , -0.1742  , -0.40666 ,  0.20348 ,\n",
       "       -0.011788,  0.48252 ,  0.024598,  0.34064 , -0.084724,  0.5324  ,\n",
       "       -0.25103 ,  0.62546 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aa1f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4465 words (2535 misses)\n"
     ]
    }
   ],
   "source": [
    "#Company GloVe embedding\n",
    "\n",
    "company_num_tokens = len(company_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "company_embedding_matrix = np.zeros((company_num_tokens, embedding_dim))\n",
    "for word, i in company_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        company_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97a42c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6167 words (833 misses)\n"
     ]
    }
   ],
   "source": [
    "#User GloVe embedding\n",
    "\n",
    "user_num_tokens = len(user_vocabulary)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare company embedding matrix\n",
    "user_embedding_matrix = np.zeros((user_num_tokens, embedding_dim))\n",
    "for word, i in user_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        user_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83be3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.48279995e-01,  1.77609995e-01,  4.23460007e-01, -3.14889997e-01,\n",
       "        3.22730005e-01, -7.24129975e-01, -7.89550006e-01,  4.92139995e-01,\n",
       "       -2.06929997e-01, -5.50879980e-04, -4.78769988e-01,  2.88529992e-01,\n",
       "       -5.73759973e-01,  2.72170007e-01,  1.11290002e+00,  5.78079998e-01,\n",
       "        6.93210006e-01, -2.86520004e-01, -5.45450002e-02, -6.18260026e-01,\n",
       "        1.72270000e-01,  2.92629987e-01,  3.81839991e-01,  6.21860027e-01,\n",
       "        5.54610014e-01, -1.74109995e+00, -2.88020015e-01, -1.71399996e-01,\n",
       "        7.47430027e-01, -1.01349998e+00,  3.35960007e+00,  1.13699996e+00,\n",
       "       -1.00279999e+00,  1.76850006e-01, -6.17949991e-03, -6.34910017e-02,\n",
       "        1.90770000e-01,  4.40459996e-02,  3.82279992e-01, -4.16070014e-01,\n",
       "       -5.03589988e-01, -8.38029981e-02,  1.75080001e-01,  4.04199988e-01,\n",
       "        7.73240030e-02,  1.74150005e-01,  1.25410005e-01, -2.18199998e-01,\n",
       "        1.29710004e-01,  3.29530001e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check user validity\n",
    "user_embedding_matrix[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e61bd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49708   ,  0.054785  ,  0.86637998,  0.46548   , -0.95643002,\n",
       "        0.08187   , -0.004151  , -0.069125  , -1.70000005,  1.26129997,\n",
       "        1.29079998, -0.14752001, -1.38170004,  0.083292  , -0.12346   ,\n",
       "       -0.33599001, -0.44850001,  0.38988   , -1.13240004, -0.36943999,\n",
       "       -0.73693001, -0.57831001, -0.61009002,  1.66980004, -0.53049999,\n",
       "       -0.12488   ,  1.15690005,  0.31060001, -0.52116001,  0.33651999,\n",
       "        1.81229997,  1.34099996,  0.23122001,  0.12511   ,  0.048984  ,\n",
       "        0.30794999,  0.67546999,  0.66725999,  0.1531    , -0.60719001,\n",
       "        2.01889992,  0.50082999, -0.73434001, -0.32253999, -0.78384   ,\n",
       "        1.16390002,  0.33465001,  0.029798  ,  0.78741002, -0.48907   ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check company validity\n",
    "company_embedding_matrix[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4d8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company embedding\n",
    "companny_embedding_layer = Embedding(\n",
    "    company_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(company_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user embedding\n",
    "user_embedding_layer = Embedding(\n",
    "    user_num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(user_embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "146b9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded_sequences = user_embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(embedding_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedded_sequences)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4504d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedded_sequences = companny_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded_sequences,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(user_num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4b5ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "229bb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     350000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     350000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 50),         20200       ['embedding_1[0][0]']            \n",
      "                                 (None, 50),                                                      \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 50),   20200       ['embedding[0][0]',              \n",
      "                                 (None, 50),                      'lstm[0][1]',                   \n",
      "                                 (None, 50)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 7000)   357000      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,097,400\n",
      "Trainable params: 397,400\n",
      "Non-trainable params: 700,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3de96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = user_vectorizer(np.array([[s] for s in train_data.User])).numpy()\n",
    "y = company_vectorizer(np.array([[s] for s in train_data.Company])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6301730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13067, 20), (13067, 20))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eddde91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = []\n",
    "for i in y:\n",
    "    train_y_final_output.append(i[1:])\n",
    "train_y_final_output = pad_sequences(train_y_final_output, 20, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48245ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " array([  1,  65,   5,  72,  61,   4,  11,   4,  16,  35,  14,   9,  17,\n",
       "        137,  95,  18,   7,  68,  47,   0]),\n",
       " (13067, 20))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y_final_output[0]), train_y_final_output[0], train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98dd284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final_output = to_categorical(train_y_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e25de013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13067, 20, 7000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9199a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "327/327 [==============================] - 22s 67ms/step - loss: 4.1550 - acc: 0.3238 - val_loss: 4.1543 - val_acc: 0.3347\n",
      "Epoch 2/15\n",
      "327/327 [==============================] - 22s 68ms/step - loss: 4.0695 - acc: 0.3350 - val_loss: 4.0794 - val_acc: 0.3398\n",
      "Epoch 3/15\n",
      "327/327 [==============================] - 22s 69ms/step - loss: 3.9970 - acc: 0.3428 - val_loss: 4.0167 - val_acc: 0.3469\n",
      "Epoch 4/15\n",
      "327/327 [==============================] - 23s 69ms/step - loss: 3.9347 - acc: 0.3490 - val_loss: 3.9791 - val_acc: 0.3526\n",
      "Epoch 5/15\n",
      "327/327 [==============================] - 22s 69ms/step - loss: 3.8893 - acc: 0.3542 - val_loss: 3.9393 - val_acc: 0.3559\n",
      "Epoch 6/15\n",
      "327/327 [==============================] - 23s 69ms/step - loss: 3.8502 - acc: 0.3588 - val_loss: 3.9209 - val_acc: 0.3573\n",
      "Epoch 7/15\n",
      "327/327 [==============================] - 23s 70ms/step - loss: 3.8162 - acc: 0.3623 - val_loss: 3.8843 - val_acc: 0.3628\n",
      "Epoch 8/15\n",
      "327/327 [==============================] - 22s 66ms/step - loss: 3.7852 - acc: 0.3661 - val_loss: 3.8618 - val_acc: 0.3677\n",
      "Epoch 9/15\n",
      "327/327 [==============================] - 22s 68ms/step - loss: 3.7550 - acc: 0.3693 - val_loss: 3.8466 - val_acc: 0.3671\n",
      "Epoch 10/15\n",
      "327/327 [==============================] - 23s 71ms/step - loss: 3.7269 - acc: 0.3726 - val_loss: 3.8190 - val_acc: 0.3711\n",
      "Epoch 11/15\n",
      "327/327 [==============================] - 23s 70ms/step - loss: 3.7029 - acc: 0.3756 - val_loss: 3.8038 - val_acc: 0.3741\n",
      "Epoch 12/15\n",
      "327/327 [==============================] - 22s 67ms/step - loss: 3.6748 - acc: 0.3785 - val_loss: 3.7864 - val_acc: 0.3771\n",
      "Epoch 13/15\n",
      "327/327 [==============================] - 23s 69ms/step - loss: 3.6534 - acc: 0.3809 - val_loss: 3.7741 - val_acc: 0.3792\n",
      "Epoch 14/15\n",
      "327/327 [==============================] - 23s 70ms/step - loss: 3.6352 - acc: 0.3835 - val_loss: 3.7664 - val_acc: 0.3801\n",
      "Epoch 15/15\n",
      "327/327 [==============================] - 22s 68ms/step - loss: 3.6195 - acc: 0.3854 - val_loss: 3.7558 - val_acc: 0.3831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1051647f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y], train_y_final_output, epochs = 15, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c99bb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights/mixed_lstm_glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4abcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= companny_embedding_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5d0ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq = user_vectorizer(input_seq)\n",
    "    #print(input_seq)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = company_word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    repeat = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = company_vocabulary[sampled_token_index]\n",
    "        prev = decoded_sentence\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (prev.rstrip() == decoded_sentence.rstrip()):\n",
    "            repeat = repeat + 1\n",
    "        else:\n",
    "            repeat = 0\n",
    "        \n",
    "        if (sampled_char == 'end' or\n",
    "           len(decoded_sentence) > 20):\n",
    "            stop_condition = True\n",
    "        if repeat > 2:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f226843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UNK] we can help with\n"
     ]
    }
   ],
   "source": [
    "print(decode_sequence([\"@AskPlayStation Thank youðŸ’–you are awesome\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ebf6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [UNK] we want to hear\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] we can be a look\n",
      " [UNK] hey there we can\n",
      " [UNK] we appreciate your\n",
      " [UNK] we can help with\n",
      " [UNK] hey there we can\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help with\n",
      " [UNK] we can help to\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] i apologize for\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] we can help you\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] i am sorry for\n",
      " [UNK] we can help you\n",
      " [UNK] hi there can you\n",
      " [UNK] we want to help\n",
      " [UNK] we can help with\n",
      " [UNK] hey there can you\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help with\n",
      " [UNK] we can help to\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help you\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] i apologize for\n",
      " [UNK] we can help with\n",
      " [UNK] i am sorry for\n",
      " [UNK] sorry to hear that\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] sorry to hear that\n",
      " [UNK] we can help with\n",
      " [UNK] we want to help\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] i am sorry for\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] we appreciate your\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] we can help you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hey there we can\n",
      " [UNK] we appreciate your\n",
      " [UNK] hi there sorry\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there can you\n",
      " [UNK] we appreciate your\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we can help you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help to\n",
      " [UNK] i apologize for\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hey there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help to\n",
      " [UNK] we want to help\n",
      " [UNK] we appreciate your\n",
      " [UNK] we appreciate your\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] we appreciate your\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help to\n",
      " [UNK] hi there can you\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] we can help with\n",
      " [UNK] we appreciate your\n",
      " [UNK] we can help with\n",
      " [UNK] i am sorry for\n",
      " [UNK] we can help with\n",
      " [UNK] hey there we can\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] hey there can you\n",
      " [UNK] we appreciate your\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help with\n",
      " [UNK] i apologize for\n",
      " [UNK] we appreciate your\n",
      " [UNK] hi [UNK] sorry\n",
      " [UNK] hey there we can\n",
      " [UNK] we want to hear\n",
      " [UNK] we can help with\n",
      " [UNK] we want to help\n",
      " [UNK] hey there we can\n",
      " [UNK] we can help you\n",
      " [UNK] we can help with\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help to\n",
      " [UNK] hey there can you\n",
      " [UNK] we can help to\n",
      " [UNK] we can help to\n",
      " [UNK] hi there sorry\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there can you\n",
      " [UNK] hi there sorry\n",
      " [UNK] we can help with\n",
      " [UNK] hi [UNK] we have\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data[:200].iterrows():\n",
    "    print(decode_sequence([row['User']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a5cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
